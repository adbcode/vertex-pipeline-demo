{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb8369d-d2aa-438f-b243-9d4b39f3d426",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Create a pipeline which:\n",
    "- Loads data from a Cloud Storage bucket\n",
    "- Train a scikit-learn model\n",
    "- Perform validation\n",
    "- Upload model\n",
    "- Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c7fd1e-33d3-476d-8ab7-2b1dbc86bf4a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- Using workbench with `Pytorch:2.0` image\n",
    "- Create bucket (ideally in same region)\n",
    "`gsutil mb -l europe-west4 gs://<BUCKET NAME>`\n",
    "- Download the test file to upload to bucket\n",
    "`wget http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv`\n",
    "- Upload training file to bucket\n",
    "`gcloud storage cp winequality-white.csv gs://<BUCKET NAME>/data/wine.csv`\n",
    "- Upload test file to bucket\n",
    "`gcloud storage cp test.csv gs://<BUCKET NAME>/data/test.csv`\n",
    "- Install `google_cloud_pipeline_components` (1.0.44)\n",
    "`pip install -q --upgrade \"google-cloud-pipeline-components<2\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e8d44-909f-43ff-8534-ebb4c587f2c2",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "616edda0-99a1-4645-b687-5ec3de744ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION=\"europe-west4\"\n",
    "PROJECT_ID=\"<PROJECT ID>\"\n",
    "BUCKET_NAME=\"gs://<BUCKET NAME>\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed7a494-8212-4d6c-b26a-200bbbd576cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import typing\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee6f57-d174-4d03-831d-8e50229db89d",
   "metadata": {},
   "source": [
    "## Step 1 - Read from cloud storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc384580-a585-42b0-b599-6fd21265a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"europe-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def load_data_from_bucket(\n",
    "    url: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    df = pd.read_csv(url, delimiter=\";\")\n",
    "    df['best_quality'] = [ 1 if x >= 7 else 0 for x in df.quality] \n",
    "    df['target'] = df.best_quality\n",
    "    df = df.drop(['quality', 'total sulfur dioxide', 'best_quality'], axis=1)\n",
    "   \n",
    "    train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    train.to_csv(dataset_train.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    test.to_csv(dataset_test.path + \".csv\" , index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da562d0-3825-454f-a20b-7efd93d91a9d",
   "metadata": {},
   "source": [
    "## Step 2 - Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "164a415c-f3a1-4e9e-809e-f15035caf528",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"europe-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def train_model(\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Model], \n",
    "):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "\n",
    "    data = pd.read_csv(dataset.path + \".csv\")\n",
    "    model_rf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    model_rf.fit(data.drop(columns=[\"target\"]), data.target)\n",
    "    model.metadata[\"model_type\"] = \"RandomForestClassifier\"\n",
    "    file_name = model.path + f\".pkl\"\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(model_rf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f8c44-6642-443c-975a-b36a9985ed9d",
   "metadata": {},
   "source": [
    "## Step 3 - Validate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e166c40-8e16-48d2-af50-6b44031f0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"europe-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def validate_model(\n",
    "    test_set: Input[Dataset],\n",
    "    trained_model: Input[Model],\n",
    "    thresholds_dict_str: str,\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    kpi: Output[Metrics]\n",
    ") -> NamedTuple(\"output\", [(\"deploy\", str)]):\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import pickle\n",
    "    from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "    import json\n",
    "    import typing\n",
    "\n",
    "    def threshold_check(val1, val2):\n",
    "        cond = \"false\"\n",
    "        if val1 >= val2 :\n",
    "            cond = \"true\"\n",
    "        return cond\n",
    "\n",
    "    data = pd.read_csv(test_set.path + \".csv\")\n",
    "    model = RandomForestClassifier()\n",
    "    file_name = trained_model.path + \".pkl\"\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        model = pickle.load(file)\n",
    "    \n",
    "    y_test = data.drop(columns=[\"target\"])\n",
    "    y_target = data.target\n",
    "    y_pred = model.predict(y_test)\n",
    "    \n",
    "    y_scores =  model.predict_proba(data.drop(columns=[\"target\"]))[:, 1]\n",
    "\n",
    "    print(\"calculating roc\")\n",
    "    roc_auc = roc_auc_score(y_target, y_scores)\n",
    "    trained_model.metadata[\"roc_auc\"] = float(roc_auc)\n",
    "    \n",
    "    print(\"calculating accuracy score\")\n",
    "    accuracy = accuracy_score(data.target, y_pred.round())\n",
    "    trained_model.metadata[\"accuracy\"] = float(accuracy)\n",
    "    kpi.log_metric(\"accuracy\", float(accuracy))\n",
    "    \n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    deploy = threshold_check(float(roc_auc), int(thresholds_dict['roc']))\n",
    "    return (deploy,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91e966e-7e2b-447a-a25b-e2200001d5c4",
   "metadata": {},
   "source": [
    "## Step 4 - Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3422417-88a3-4d57-a86e-8d2d1774e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform<2\"],\n",
    "    base_image=\"europe-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest\",\n",
    ")\n",
    "def deploy_model(\n",
    "    model: Input[Model],\n",
    "    model_name: str,\n",
    "    project: str,\n",
    "    region: str,\n",
    "    serving_container_image_uri : str, \n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project, location=region)\n",
    "\n",
    "    DISPLAY_NAME = model_name\n",
    "    MODEL_NAME = f\"{model_name}_rf\"\n",
    "    ENDPOINT_NAME = f\"{model_name}_endpoint\"\n",
    "    \n",
    "    def create_endpoint():\n",
    "        endpoints = aiplatform.Endpoint.list(\n",
    "            filter='display_name=\"{}\"'.format(ENDPOINT_NAME),\n",
    "            order_by='create_time desc',\n",
    "            project=project,\n",
    "            location=region\n",
    "        )\n",
    "        if len(endpoints) > 0:\n",
    "            endpoint = endpoints[0]  # most recently created\n",
    "        else:\n",
    "            endpoint = aiplatform.Endpoint.create(\n",
    "                display_name=ENDPOINT_NAME, project=project, location=region\n",
    "            )\n",
    "    endpoint = create_endpoint()   \n",
    "    \n",
    "    #Import a model programmatically\n",
    "    model_upload = aiplatform.Model.upload(\n",
    "        model_id = DISPLAY_NAME,\n",
    "        artifact_uri = model.uri[:-5],\n",
    "        serving_container_image_uri = serving_container_image_uri,\n",
    "        serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    "        serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "        serving_container_environment_variables={\"MODEL_NAME\": MODEL_NAME}\n",
    "    )\n",
    "    model_deploy = model_upload.deploy(\n",
    "        machine_type=\"n1-standard-4\", \n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=DISPLAY_NAME\n",
    "    )\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_model.uri = model_deploy.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f6a91-249e-4561-b5ed-efc40276c01b",
   "metadata": {},
   "source": [
    "## Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02ab89f1-6f42-4b18-b631-e59bed4c53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DISPLAY_NAME = 'pipeline-test-job{}'.format(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0115c7e5-9a78-42aa-b981-39d53da943cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"pipeline-test\"\n",
    ")\n",
    "def pipeline(\n",
    "    url: str = \"/gcs/<BUCKET NAME>/data/wine.csv\",\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION,\n",
    "    thresholds_dict_str: str = '{\"roc\":0.8}',\n",
    "    serving_container_image_uri: str = \"europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    "    ):\n",
    "    \n",
    "    data_op = load_data_from_bucket(url)\n",
    "    train_model_op = train_model(data_op.outputs[\"dataset_train\"])\n",
    "    model_evaluation_op = validate_model(\n",
    "        test_set=data_op.outputs[\"dataset_test\"],\n",
    "        trained_model=train_model_op.outputs[\"model\"],\n",
    "        thresholds_dict_str=thresholds_dict_str,\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(\n",
    "        model_evaluation_op.outputs[\"deploy\"]==\"true\",\n",
    "        name=\"deploy-test\",\n",
    "    ):     \n",
    "        deploy_model_op = deploy_model(\n",
    "            model=train_model_op.outputs['model'],\n",
    "            model_name=\"pipeline-test\",\n",
    "            project=project,\n",
    "            region=region,\n",
    "            serving_container_image_uri = serving_container_image_uri\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd9f929-fefd-4a0e-a19e-6ff5512f8581",
   "metadata": {},
   "source": [
    "## Compile and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "253588a5-b308-40fa-82ef-aba66588526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='pipeline_model.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a7db6a1-bda6-4a46-963f-a56e6d97dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"pipeline-test\",\n",
    "    template_path=\"pipeline_model.json\",\n",
    "    enable_caching=False,\n",
    "    location=REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59ad2a30-a3d5-4ade-8442-bfa57646b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f66bf-916b-4827-a866-49a806f23f00",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "351ca7ff-edce-4dc7-857c-686864370c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://europe-west4-aiplatform.googleapis.com/]\n",
      "MODEL_ID             DISPLAY_NAME\n",
      "3730097595276591104  pipeline-test\n",
      "5058659485350887424  pipeline-test\n",
      "5538855795619266560  pipeline-test\n"
     ]
    }
   ],
   "source": [
    "! gcloud ai models list --region={REGION} --filter=\"pipeline-test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db574ce-969c-4007-85e0-7e27eabee993",
   "metadata": {},
   "source": [
    "## Batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "286bc276-4799-4c89-8fc8-76dca03803e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline-batch-prediction-job\n",
      "JobState.JOB_STATE_SUCCEEDED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.jobs.BatchPredictionJob object at 0x7fbcdab060e0> \n",
       "resource name: projects/840635250828/locations/europe-west4/batchPredictionJobs/1160580152850120704"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define variables \n",
    "job_display_name = \"pipeline-batch-prediction-job\"\n",
    "MODEL_NAME=\"pipeline-test\"\n",
    "ENDPOINT_NAME=f\"{MODEL_NAME}_endpoint\"\n",
    "BUCKET_URI=\"gs://<BUCKET NAME>/data\"\n",
    "input_file_name=\"test.csv\"\n",
    "\n",
    "# Get model id\n",
    "MODEL_ID=!(gcloud ai models list --region=$REGION \\\n",
    "           --filter=display_name=$MODEL_NAME)\n",
    "MODEL_ID=MODEL_ID[2].split(\" \")[0]\n",
    "\n",
    "model_resource_name = f\"projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}\"\n",
    "gcs_source= [f\"{BUCKET_URI}/{input_file_name}\"]\n",
    "gcs_destination_prefix=f\"{BUCKET_URI}/output\"\n",
    "\n",
    "def batch_prediction_job(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_resource_name: str,\n",
    "    job_display_name: str,\n",
    "    gcs_source: str,\n",
    "    gcs_destination_prefix: str,\n",
    "    machine_type: str,\n",
    "    starting_replica_count: int = 1, # The number of nodes for this batch prediction job. \n",
    "    max_replica_count: int = 1,    \n",
    "):   \n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    model = aiplatform.Model(model_resource_name)\n",
    "\n",
    "    batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        instances_format=\"csv\", #json\n",
    "        gcs_source=[f\"{BUCKET_URI}/{input_file_name}\"],\n",
    "        gcs_destination_prefix=f\"{BUCKET_URI}/output\",\n",
    "        machine_type=machine_type, # must be present      \n",
    "    )\n",
    "    batch_prediction_job.wait()\n",
    "    print(batch_prediction_job.display_name)\n",
    "    print(batch_prediction_job.state)\n",
    "    return batch_prediction_job\n",
    "\n",
    "batch_prediction_job(PROJECT_ID, REGION, model_resource_name, job_display_name, gcs_source, gcs_destination_prefix, machine_type=\"n1-standard-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e7991-1099-4939-bc54-038d6cf3ac1b",
   "metadata": {},
   "source": [
    "## Using Endpoint for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ce38c3e-1438-4975-a69c-b41c748fb216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[0.0, 0.0, 0.0, 0.0, 0.0], deployed_model_id='6636392700511780864', model_version_id='1', model_resource_name='projects/840635250828/locations/europe-west4/models/3730097595276591104', explanations=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = [[1,2,3,2,1,2,3,6,7,10],\n",
    "            [6.2,0.33,0.14,4.8,0.052,27,0.99475,3.21,0.48,9.4],\n",
    "            [7.1,0.21,0.27,8.6,0.056,26,0.9956,2.95,0.52,9.5],\n",
    "            [6.1,0.16,0.24,1.4,0.046,17,0.99319,3.66,0.57,10.3],\n",
    "            [6.3,0.29,0.23,14.2,0.037,24,0.99528,3.08,0.38,10.6]\n",
    "           ]\n",
    "ENDPOINT_ID = !(gcloud ai endpoints list --region=$REGION \\\n",
    "              --format='value(ENDPOINT_ID)'\\\n",
    "              --filter=display_name=$ENDPOINT_NAME \\\n",
    "              --sort-by=creationTimeStamp | tail -1)\n",
    "ENDPOINT_ID = ENDPOINT_ID[1]\n",
    "\n",
    "def endpoint_predict(\n",
    "    project: str, location: str, instances: list, endpoint: str\n",
    "):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    endpoint = aiplatform.Endpoint(endpoint)\n",
    "\n",
    "    prediction = endpoint.predict(instances=instances)\n",
    "    return prediction\n",
    "\n",
    "endpoint_predict(PROJECT_ID, REGION, instance, ENDPOINT_ID)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.2-0.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.2-0:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
